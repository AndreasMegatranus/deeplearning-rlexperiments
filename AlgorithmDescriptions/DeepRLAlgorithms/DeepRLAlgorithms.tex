%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmlarge,screen]{acmart}

%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2021}
\acmYear{2021}
% \setcopyright{acmlicensed}
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmDOI{10.1145/1122445.1122456}
% \acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

\DeclareMathOperator*{\argmax}{argmax}

\graphicspath{{Figures/}} % Set the default folder for images
\usepackage{algorithm}
\usepackage{algpseudocode}


%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Deep Reinforcement Learning Algorithms}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Andreas Hofmann}
\email{hofma@csail.mit.edu}

%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Hofmann}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
  This provides a summary and references for current standard \textit{Deep Reinforcement Learning} (DRL) algorithms.
  The emphasis is on DRL for control of plants (agents) with continuous control actions, particularly, robots.
  As this field becomes more mature, it is important to be aware of open source tools that simplify implementation and use of
  the algorithms
  Therefore, in addition to the algorithm descriptions, this document includes summaries of important tools,
  including simulators, deep learning platforms like PyTorch, and a variety of wrappers for platforms like PyTorch that make
  implementation easier.
\end{abstract}


%
% A "teaser" image appears between the author and affiliation information and the body 
% of the document, and typically spans the page. 
%%\begin{teaserfigure}
%%  \includegraphics[width=\textwidth]{sampleteaser}
%%  \caption{Seattle Mariners at Spring Training, 2010.}
%%  \Description{Enjoying the baseball game from the third-base seats. Ichiro Suzuki preparing to bat.}
%%  \label{fig:teaser}
%%\end{teaserfigure}

%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
% Don't include acm ref.  -agh, 8/12/2021
\settopmatter{printacmref=false}  
\maketitle

\section{Introduction}

To begin, it is useful to understand the place of DRL within the general field of Deep Learning.
The field of Deep Learning can be divided into three categories: supervised learning, unsupervised learning, and
reinforcement learning.

Supervised learning requires first generating a possibly large \textit{training set} of example inputs with corresponding outputs.
The outputs are sometimes called ``labels''.
For example, in an image classification problem, the inputs are images, and the outputs are associated labels that
descibe the images.
The generation of such training sets is typically the most time consuming aspect of supervised learning.

Unsupervised learning avoids the need for ``supervision'' by not requiring labels.
It uses input (a set of images, for example), and tries to find hidden structure and relations.
A typical task is to cluster inputs into sets based on some aspect of similarity.

Reinforcement learning can be considered to be ``between'' supervised and unsupervised learning in terms of the human supervision
that it requires.
Although reinforcement learning does not require direct supervision in the form of labels, it does require an environment for the
\textit{agent} to explore, and a reward function based on the agent's behavior.
The environment is often simulated.
Fidelity and speed of the simulation are key determinants of success.





Intro sub sections

Overview of other sections

Use book as guide for how to organize, but bias towards continuous action spaces.





\subsection{Algorithm Categorization}

DRL (Deep Reinforcement Learning) Algorithms can be categorized according to a number of characteristics.
These include whether the algorithm 

\begin{itemize}
\item uses a model (\textit{model-based} vs. \textit{model-free});
\item learns a value function or learns a policy directly;
\item is an \textit{on-policy} or \textit{off-policy} algorithm.
\end{itemize}

With model-based RL, a model of state transition (next state given current state
and action) is used.  The model supports searching multiple steps ahead in order to select the optimal action,
based on reward at each step.
This search can be informed by both value and policy heuristics, as was done in Alpha Go [cite].
With model-free RL, no explicit state transition model is used.
Instead, one or both of two simpler functions (value function and/or policy function) is learned directly.

A value function returns a scalar value for a given current state.  Related to this, a Q function (or value-action function)
returns a scalar value given a current state and a proposed action from this state.  
These scalar values represent the discounted reward, or future utility, that is expected to accrue, if a particular control
policy is followed.  The Q function can be used to pick the best action given a current state (the action that maximizes reward).
A policy function directly returns an action for a given current state.
Ideally, the policy function is learned well enough that it closely approximates the \textit{optimal} policy function
(returns the best action for a given current state).

An \textit{actor-critic} approach uses both value and policy functions.

In order to understand the difference between on-policy and off-policy methods, it is important to consider and separate out
the policy being learned and the policy used to generate the data.
For on-policy methods, these are the same.
For off-policy methods, these are not the same.
For example, in Q-learning (value function and Q function learning), the policy used to control the agent in the environment
(to generate the data) is only sometimes the policy being learned.  
In particular, the action is sometimes chosen from the policy being learned (greedy option), and is sometimes chosen randomly
(exploratory option).  
This helps to ensure that the learning does not prematurely converge to a local optimum that is significantly less optimal
than the global optimum.
Selection of whether to choose the greedy or exploratory action is based on an \textit{epsilon} factor that decays over time, 
so that as the policy is learned, selection of the greedy action becomes more likely
(leveraging the fact that the policy should be getting better over time).

Q-Learning is an example of an off-policy method.
SARSA is an example of an on-policy method.
See also \url{https://analyticsindiamag.com/reinforcement-learning-policy}.


\noindent See also 
\url{https://smartlabai.medium.com/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc}

\url{https://www.guru99.com/reinforcement-learning-tutorial.html}

\url{https://en.wikipedia.org/wiki/Deep_reinforcement_learning#Off-policy_reinforcement_learning}

\url{https://analyticsindiamag.com/reinforcement-learning-policy}.


Additional characteristics used to categorize DRL algorithms include:

\begin{itemize}
\item whether the output action is discrete or continuous;
\item whether the input is raw pixels or a state estimate vector;
\item whether the system is deterministic or stochastic.
\end{itemize} 

[Investigate whether there are algorithms that have hybrid discrete/continuous output actions.]

In principle, DRL algorithms model Markov Decision Processes (MDPs).
As such, they are based on the notion of state.
However, in many cases, the distinction between state and sensor data is blurred.
For example, in many DRL algorithms, an environment like the cart-pole environment in the Open AI gym 
provides pixels as the state, rather than the traditional position and angle state estimate, even
though, from a traditional control systems standpoint, pixels should be regarded as sensor input.
This blurring can be considered a feature as it allows for end to end control algorithms.

Regarding whether the system is deterministic or stochastic, a general MDP supports a stochastic environment,
where a particular control action given a current state implies a distribution of possible next states.
The deterministic case is just the degenerate case of the MDP, where the control action implies a single next state.
There is some confusion about this terminology in some of the DRL algorithms;  in some cases, stochastic
implies a control policy that provides a distribution of possible actions given the current state.
Thus, it is possible to have both a distribution of possible actions, and a distribution of next states
conditioned for each action.



\subsection{Algorithm Summaries}


Intro to algorithms (summary of each)
List algorithms, organized (see web sites)






deterministic vs. stochastic, MDP

Confusion about this also






\subsection{To Do}

Focus on continuous action models like mountain car.

Think about whether it makes sense to have hybrid discrete/continuous actions, maybe
from two separate heads of the DNN.



\section{Background}

Problem statement, take from below.



\section{DQN}

Q value algorithm


\subsection{Agent, Reward, Value Function, Q Function, and Policy}

In Reinforcement Learning, an \textit{agent} takes actions in an environment in order to accumulate \textit{rewards}.
The immediate reward at time increment $t$ is given as $R_t$.
When learning how an agent should behave, it is useful to consider \textit{cumulative} reward, 
rather than just immediate reward.
Cumulative reward is defined as the total reward accumulated by the agent over some time horizon:

\begin{equation}
  C_t = R_t + R_{t+1} + R_{t+2} + \ldots = \sum_{k=0}^T R_{t+k} 
\end{equation}

Here, the agent takes actions at regular time increments, denoted by $k$.  
The time horizon, $T$, is specified in terms of the maximum value for $k$.

In practice, when learning agent behaviors, it is useful to favor immediate reward over future reward. 
This is expressed as \textit{discounted} reward, using a discount rate, $\gamma$:
  
\begin{equation}
  G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots = \sum_{k=0}^T \gamma^k R_{t+k} 
\end{equation}

With the discount rate, $T$ is usually set at infinity

\begin{equation}
  G_t = \sum_{k=0}^\infty \gamma^k R_{t+k} 
\end{equation}

The discount rate implies a time horizon in that rewards diminish significantly in the distant future.

A policy, $\pi$, specifies an action, $a$, to be taken by the agent when the state is $s$.  
Two important functions can be defined with respect to a policy.
First, a value function gives the total expected reward given a current state, $s_t$, 
if a particular policy, $\pi$, is followed.

\begin{equation}
  v_\pi (s) = \mathbb{E} \left( G_t \vert S_t = s \right)
\end{equation}

Second, an action-value or \textit{Q} function gives the total expected reward given a current state, $s_t$, 
a current action, $A_t$,
and a particular policy, $\pi$, to be followed (for future actions).

\begin{equation}
  q_\pi (s,a) = \mathbb{E} \left( G_t \vert S_t = s, A_t = a \right)
\end{equation}

The policy can be expressed in terms of the Q function as

\begin{equation}
  \pi(s) = \argmax_a q_\pi (s,a)
\end{equation}

Note that this is circular;  the Q function is, itself, a function of the policy.  
Thus, the challenge is how to compute (learn) the Q function.
To accomplish this, we leverage the Bellman equation:

\begin{equation} \label{eq:tempdiff1}
  q_\pi (s,a) = r + \gamma q_\pi (s^\prime, \pi(s^\prime)) = r + \gamma \argmax_{a^\prime} q_\pi (s^\prime, a^\prime)
\end{equation}

\noindent where $r$ is the immediate reward for action $a$, $s^\prime$ is the next state resulting from taking
action $a$ at state $s$, and $a^\prime$ is the action to be taken at state $s^\prime$.



\subsection{Training: Loss Function, Agent Steps, Replay Memory, and Policy Update}


The \textit{temporal difference error}, $\delta$, is defined as the difference between the two sides of \ref{eq:tempdiff1}:

\begin{equation} \label{eq:tempdiff1}
 \delta =  q_\pi (s,a) = - \left( r + \gamma \argmax_{a^\prime} q_\pi (s^\prime, a^\prime) \right)
\end{equation}

During training, RL algorithms strive to update $q_\pi (s,a)$ so that $\delta$ approaches 0.
In (non-deep) RL, $q_\pi (s,a)$ is represented as a table, and is updated by:

\begin{equation} \label{eq:qupdate}
 q_\pi (s,a) = q_\pi (s,a) + \alpha \left( r + \gamma \argmax_{a^\prime} q_\pi (s^\prime, a^\prime) - q_\pi (s,a) \right)
\end{equation}

\noindent where $\alpha$ is the learning rate.

In Deep Reinforcement Learning, $q_\pi (s,a)$ is represented using a Deep Neural Net rather than a table.
Training (minimization of $\delta$) is accomplished by defining a loss based on $\delta$.
In the PyTorch DQN tutorial (\url{https://pytorch.org/tutorials/intermediate/reinforcement\_q\_learning.html}),
a \textit{Huber Loss} function is used.
This loss, $\mathcal{L}$, is defined as 


\begin{equation}  \label{eq:huberloss}
  \mathcal{L} =
    \begin{cases}
      \frac{1}{2} \delta^2 & \text{for $\lvert \delta \rvert \leq 1$,}\\
      \lvert \delta \rvert - \frac{1}{2} & \text{otherwise.}
    \end{cases}       
\end{equation}

Training occurs over some specified number of episodes (50, for example).
Within each episode, the environment state is first re-initialized, and then the agent explores the environment by taking
\textit{steps}.
For each step, the agent takes an action, resulting in a reward, and a transition from the current state to the next state.
Thus, a step can be summarized by the tuple $\left< \text{state action next-state reward} \right>$.
An episode ends when the environment decides this (usually because the agent has entered a failure state), 
or at some specified limit on the number of steps per episode.

After each step, the result tuple is entered into \textit{replay memory}.  
Replay memory is a buffer of these tuples with a limited size (10000 for example).
As result tuples are added to the end of replay memory, old results are dropped from the beginning.
Thus, replay memory stores the last n (10000 for example) results of experiment steps taken by the agent.

After each agent step, the Q model is updated (optimized).
This is accomplished not by using the results from the most recent step, but rather, by sampling a \textit{batch}
of results from replay memory. 
A typical batch size is 128.
Leveraging the Huber loss function defined above, the loss for the entire batch is defined as

\begin{equation}  \label{eq:batchloss}
  \mathcal{L}_B = \frac{1}{\lvert B \rvert} \sum_{ \left( s,a,s^\prime , r \right) \in B}  \mathcal{L}
\end{equation}

\noindent Model weights are then updated using standard back propagation, based on this loss for the batch.

Experience replay memory ensures that the experience samples in a batch are not correlated;
that they are not overly biased towards results from the most recent policy (model weights).
This stabilizes and improves DQN training.

As in traditional reinforcement learning, the action selected by an agent during a step is sometimes 
\textit{greedy} (following the current learned policy), and is sometimes \textit{exploratory} (random).
The probability of selecting a random action starts high (at 0.9, for example), and then decays exponentially to
a small value (0.05, for example).



\subsection{Policy and Target Networks}

During training, two parallel networks are used:  \textit{policy} network, and \textit{target} network.  
The policy network weights are updated at each training step, using standard back propagation.
The target network weights are kept fixed for most steps;  they are updated at a specified interval
(every 10 training steps for example) from the policy network weights.
The policy network is used to compute the left side of \ref{eq:tempdiff1}.  
The target network is used to compute $q_\pi (s^\prime, a^\prime)$ for the right side of \ref{eq:tempdiff1}.  
Using a separate target network in this way improves training stability.


\section{Policy-Based Methods}

This is based on the excellent tutorial in Hugging Face:
\url{https://huggingface.co/learn/deep-rl-course/unit4/introduction}

See also DRL Hands On, and Open AI ref that Kamil sent, and, of course, the original papers.

\subsection{Value-Based and Policy-Based Methods}

In value-based methods, the policy, $\pi$, is not represented explicitly.
Instead, the action is computed from the value function (a function that gives the value of the current state)
or the Q function (a function that gives the value of a proposed action given the current state).
In policy-based methods, the action is computed directly from a policy function, which is a function of the current state.

In Deep RL, value or Q functions are implemented using a deep neural network.
Similarly, policy functions are implemented using a deep NN.
Actor-critic methods use a combination of value and policy based NN models.

The policy function is typically written as $\pi_\theta$ where $\theta$ represents the NN weights (parameters).
The policy function typically represents a \textbf{stochastic} policy;  the policy function outputs a
probability distribution over actions, rather than a deterministic value.

\begin{equation}
  \pi_\theta (s) = \Pr \left[ A \mid s; \theta \right]
\label{eq:stochasticpolicy1}
\end{equation}


A typical architecture for a stochastic policy model is shown in Figure \ref{fig:StochasticPolicyArchitecture11}.
Compared with neural net architectures used for image recognition, which have large numbers of layers including convolutional layers for filtering,
the architectures used in deep reinforcement learning are relatively simple.
The example shown here is typical;  there is an input layer, a single hidden fully connected layer, and an output layer, which is a softmax layer
(see \url{https://en.wikipedia.org/wiki/Softmax_function}).
The current state is input to the input layer.
The output is a set of probabilities for (in this case) discrete actions.


\begin{figure}[tb]
\centering 
\includegraphics[width=0.7\columnwidth]{stochasticpolicyarchitecture} 
\caption[Stochastic policy architecture]{Neural net architecture for parameterized stochastic policy} % The text in the square bracket is the caption for the list of figures while the text in the curly brackets is the figure caption
\label{fig:StochasticPolicyArchitecture11} 
\end{figure}


\subsection{Training a policy model}

Models are trained by adjusting the model parameters, $\theta$, so that an appropriate objective function, $J(\theta)$, is maximized.
In this case, the objective function is the expected reward;  we want to maximize the expected reward in an RL system.
Specifically, the policy is to be tuned so that actions that maximize the return (given the current state) have higher probability.
The algorithm for this is as follows:

\begin{algorithm}
\caption{Policy model training algorithm}\label{alg:policytraining}
\begin{algorithmic}[1]
  \State Collect an episode with the current policy
  \State Calculate the reward for the episode
  \State Update the model parameters:
  \If{reward is positive}
    \State increase the probability of state-action pairs taken during the episode
  \ElsIf{reward is negative}
    \State decrease the probability of state-action pairs taken during the episode
  \EndIf
\end{algorithmic}
\end{algorithm}

The objective function can be expressed as

\begin{equation}
  J(\theta) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \right]
\label{eq:stochasticpolicy1}
\end{equation}

\noindent where $\tau$ is a trajectory (sequence of states and actions) that is sampled from $\pi$ as the state evolves, and
$R$ is the cumulative reward (return).  The cumulative reward is defined as

\begin{equation}
  R(\tau) = r_{t + 1} + \gamma r_{t+2} + \gamma^2 r_{t + 3} + ...
\label{eq:cumulativereward}
\end{equation}

\noindent where $r_t$ is the reward at time $t$, and $\gamma$ is a discount factor.  Thus, future reward is counted less than
immediate reward.


The expected return for a specific trajectory, $\tau$, is given by

\begin{equation}
  \mathbb{E} \left[ R(\tau) \right] = \Pr (\tau; \theta) R(\tau)
\label{eq:expectedreturn1}
\end{equation}

The objective function can then be expressed as

\begin{equation}
  J(\theta) = \sum_{\tau} \Pr (\tau; \theta) R(\tau)
\label{eq:stochasticpolicy2}
\end{equation}

\noindent Note that this is a sum over all possible trajectories, $\tau$, which is usually impractical.
Instead, for practical purposes, a set of $N$ trajectories is sampled by running $N$ episodes.

The probability of a trajectory is a function of the policy and the environment dynamics:

\begin{equation}
  \Pr (\tau; \theta) = \prod_{t=0} \Pr(s_{t+1} \mid s_t, a_t) \pi_\theta (a_t \mid s_t)
\label{eq:trajectoryprobability}
\end{equation}

\noindent If the environment is deterministic, then

\begin{equation}
  \Pr(s_{t+1} \mid s_t, a_t) = 1
\label{eq:deterministicenvironment}
\end{equation}

\subsubsection{Gradient ascent and the Policy Gradient Theorem}

As discussed previously, the policy model is learned by adjusting $\theta$ so that the objective function, $J(\theta)$ is maximized.
The algorithm for adjusting $\theta$ is called gradient ascent.
The algorithm proceeds as a sequence of steps, where each step performs the following update:

\begin{equation}
  \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
\label{eq:gradientascentupdatestep}
\end{equation}

\noindent where $\alpha$ is a learning rate constant.

There is a problem with computing the gradient of $J(theta)$, even if we use the approximation based on a set of sampled trajectories.
For non-deterministic environments, the environment dynamics state distribution, $\Pr(s_{t+1} \mid s_t, a_t)$, may not be known, analytically.
Fortunately, this can be resolved by using the Policy Gradient Theorem, which states that:

\begin{equation}
  \nabla_\theta J (\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (a_t \mid s_t) R(\tau)  \right]
\label{eq:policygradienttheorem1}
\end{equation}

The derivation for this theorem will be presented shortly.
First, its use in a simple policy-gradient method called REINFORCE will be explained.
At each step of the REINFORCE training algorithm, the current policy, $\pi_\theta$ is used to collect a trajectory, $\tau$, for a single episode.
This is then used to estimate the gradient, $\hat{g} \approx \nabla_\theta J (\theta)$.

\begin{equation}
  \nabla_\theta J (\theta) \approx \hat{g} = \sum_{t=0} \nabla_\theta \log \pi_\theta (a_t \mid s_t) R(\tau)
\label{eq:policygradienttheorem1}
\end{equation}

\noindent The gradient estimate is then used to update $\theta$ using \ref{eq:gradientascentupdatestep};  $\theta \leftarrow \theta + \alpha \hat{g}$.

The gradient, $\nabla_\theta \log \pi_\theta (a_t \mid s_t)$, is the direction of the steepest increase of the (log) probability of selection action $a_t$ given state $s_t$.
The return, $R(\tau)$, provides a scaling factor indicating how far to go in the direction.
If $R(\tau)$ is positive, $\theta$ will be adjusted to increase the probabilities of the (state, action) combinations.
If $R(\tau)$ is negative, $\theta$ will be adjusted to decrease the probabilities of the (state, action) combinations.

It is also possible to collect multiple episodes, rather than just one, during a training step.
The gradient estimate is then just the average over all the episode trajectory gradient estimates.

\subsubsection{Derivation of the Policy Gradient Theorem}

From \ref{eq:stochasticpolicy2}, we have

\begin{equation}
  \nabla_\theta J(\theta) = \nabla_\theta \sum_{\tau} \Pr (\tau; \theta) R(\tau)
\label{eq:policygradienttheoryderivation1}
\end{equation}

\noindent The gradient of the sum on the right side can be rewritten as the sum of the gradient

\begin{equation}
  \nabla_\theta \sum_{\tau} \Pr (\tau; \theta) R(\tau) = \sum_{\tau} \nabla_\theta \left( \Pr (\tau; \theta) R(\tau) \right) = \sum_{\tau} \nabla_\theta \Pr (\tau; \theta) R(\tau)
\label{eq:policygradienttheoryderivation2}
\end{equation}

\noindent ($R(\tau)$ is not dependent on $\theta$).
The next step is to multiply the term in the sum by

\begin{equation}
  \frac{\Pr (\tau; \theta)}{\Pr (\tau; \theta)} = 1
\label{eq:policygradienttheoryderivation3}
\end{equation}

\noindent resulting in

\begin{equation}
  \sum_{\tau} \nabla_\theta \Pr (\tau; \theta) R(\tau) = \sum_{\tau} \frac{\Pr (\tau; \theta)}{\Pr (\tau; \theta)} \nabla_\theta \Pr (\tau; \theta) R(\tau) = \sum_{\tau} \Pr (\tau; \theta) \frac{\nabla_\theta \Pr (\tau; \theta)}{\Pr (\tau; \theta)} R(\tau)
\label{eq:policygradienttheoryderivation4}
\end{equation}

\noindent Now, from calculus,

\begin{equation}
  \nabla_x \log f(x) = \frac{\nabla_x f(x)}{f(x)}
\label{eq:policygradienttheoryderivation5}
\end{equation}

\noindent Therefore, \ref{eq:policygradienttheoryderivation4} can be rewritten as

\begin{equation}
  \sum_{\tau} \Pr (\tau; \theta) \frac{\nabla_\theta \Pr (\tau; \theta)}{\Pr (\tau; \theta)} R(\tau) = \sum_{\tau} \Pr (\tau; \theta) \nabla_\theta \log \Pr (\tau; \theta) R(\tau) = \nabla_\theta J(\theta)
\label{eq:policygradienttheoryderivation6}
\end{equation}








It is possible to simplify the term $\nabla_\theta \log \Pr (\tau; \theta)$.
From \ref{eq:trajectoryprobability},

\begin{equation}
  \nabla_\theta \log \Pr (\tau; \theta) = \nabla_\theta \log \left[ \mu(s_0) \prod_{t=0}^H  \Pr(s_{t+1} \mid s_t, a_t) \pi_\theta (a_t \mid s_t)   \right]    
\label{eq:policygradienttheoryderivation7}
\end{equation}

\noindent where $\mu(s_0)$ is the initial state probability, and $H$ is the trajectory horizon.

Now, the log of a product is equal to the sum of the logs.  Therefore, \ref{eq:policygradienttheoryderivation7} can be rewritten as

\begin{equation}
  \nabla_\theta \log \Pr (\tau; \theta) = \nabla_\theta \left[ \log \mu(s_0) + \sum_{t=0}^H \log \Pr(s_{t+1} \mid s_t, a_t) + \sum_{t=0}^H \log \pi_\theta (a_t \mid s_t)  \right]
\label{eq:policygradienttheoryderivation8}
\end{equation}

\noindent Also, the gradient of a sum of terms is equal to the sum of the gradients of the terms, so this can be rewritten as

\begin{equation}
  \nabla_\theta \log \Pr (\tau; \theta) = \nabla_\theta \log \mu(s_0) + \nabla_\theta \sum_{t=0}^H \log \Pr(s_{t+1} \mid s_t, a_t) + \nabla_\theta \sum_{t=0}^H \log \pi_\theta (a_t \mid s_t)
\label{eq:policygradienttheoryderivation9}
\end{equation}

\noindent Now, the initial state probability, and the environment MDP state transition dynamics are not dependent on $\theta$, so the derivative of these terms is 0.
Therefore,

\begin{equation}
  \nabla_\theta \log \Pr (\tau; \theta) =  \nabla_\theta \sum_{t=0}^H \log \pi_\theta (a_t \mid s_t)
\label{eq:policygradienttheoryderivation10}
\end{equation}

\noindent The gradient of the sum is equal to the sum of the gradients, so this can be rewritten as

\begin{equation}
  \nabla_\theta \log \Pr (\tau; \theta) =  \sum_{t=0}^H \nabla_\theta \log \pi_\theta (a_t \mid s_t)
\label{eq:policygradienttheoryderivation10}
\end{equation}













\section{DRL Hands On Chapter Notes}

[Mention which chapters are relevant]

\subsection{Chapter 17 Notes}


\subsection{Chapter 19 Notes}

For equations, utilize very simple examples to understand


Note:  hugging face has good tutorials

Hugging face tutorial is great!





\section{Experimentation}

This section describes relevant experiments, including how to set up the software and data, and result summaries.

\subsection{Experimentation with Deep Reinforcement Learning Hands On Second Edition}

This book [cite] and associated software is a great starting point for experimentation with DRL.

\subsubsection{Installation}

The code is available on Github.  See:

\noindent \url{https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition}

Just clone into a directory using ssh, as usual.

Additional requirements are

PyTorch
CUDA (if available)
Gymnasium
note, using Gymnasium, not Gym (Gymnasium is the supported descendant)
tensorboardX
tensorflow

To install Gymnasium, do

python3 -m pip install gymnasium

See github

https://github.com/Farama-Foundation/Gymnasium

This installs the basic environments.  For atari games, do

python3 -m pip install gymnasium[atari]
python3 -m pip install gymnasium[accept-rom-license]



To install tensorboardX, tensorflow, do

python3 -m pip install tensorboardX
python3 -m pip install tensorflow

To install ignite, do

python3 -m pip install pytorch-ignite


See requirements.txt for a full list


To run the Python programs in the chapters, do

python <program name>.py

Depending on the Python installation, may have to do python3 instead of python


\paragraph{Experiments from Chapter 2} The programs in this chapter are basic examples of agents operating in environments.

\underline{01\_agent\_anatomy.py}

This runs without any modifications needed.

\underline{02\_cartpole\_random.py}

This required some minor modifications, including ones due to the API change in env.step().

See

\noindent \url{https://stackoverflow.com/questions/73195438/openai-gyms-env-step-what-are-the-values}

\noindent \url{https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google}

\noindent \url{https://aleksandarhaber.com/cart-pole-control-environment-in-openai-gym-gymnasium-introduction-to-openai-gym/}


\paragraph{Experiments from Chapter 4} The programs in this chapter are basic examples using the cross-entropy learning method.

To run tensorboard, do (from the chapter level directory)

tensorboard --logdir runs

Then open a browser and use the url http://localhost:6006




\subsection{Template Styles}


\begin{verbatim}
  \documentclass[STYLE]{acmart}
\end{verbatim}

Journals use one of three template styles. All but three ACM journals use the {\verb|acmsmall|} template style:
\begin{itemize}
\item {\verb|acmsmall|}: The default journal template style.
\item {\verb|acmlarge|}: Used by JOCCH and TAP.
\item {\verb|acmtog|}: Used by TOG.
\end{itemize}


\section{Tables}

The ``\verb|acmart|'' document class includes the ``\verb|booktabs|'' package --- \url{https://ctan.org/pkg/booktabs} --- for preparing high-quality tables. 

Table captions are placed {\it above} the table.

Because tables cannot be split across pages, the best placement for them is typically the top of the page nearest their initial cite.  To ensure this proper ``floating'' placement of tables, use the environment \textbf{table} to enclose the table's contents and the table caption.  The contents of the table itself must go in the \textbf{tabular} environment, to be aligned properly in rows and columns, with the desired horizontal and vertical rules.  Again, detailed instructions on \textbf{tabular} material are found in the \textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which Table~\ref{tab:freq} is included in the input file; compare the placement of the table here with the table in the printed output of this document.

\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's live area, use the environment \textbf{table*} to enclose the table's contents and the table caption.  As with a single-column table, this wide table will ``float'' to a location deemed more desirable. Immediately following this sentence is the point at which Table~\ref{tab:commands} is included in the input file; again, it is instructive to compare the placement of the table here with the table in the printed output of this document.

\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}

\section{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of
the three are discussed in the next sections.

\subsection{Inline (In-text) Equations}
A formula that appears in the running text is called an
inline or in-text formula.  It is produced by the
\textbf{math} environment, which can be
invoked with the usual \texttt{{\char'134}begin\,\ldots{\char'134}end}
construction or with the short form \texttt{\$\,\ldots\$}. You
can use any of the symbols and structures,
from $\alpha$ to $\omega$, available in
\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a
few examples of in-text equations in context. Notice how
this equation:
\begin{math}
  \lim_{n\rightarrow \infty}x=0
\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsection{Display Equations}
A numbered display equation---one set off by vertical space from the
text and centered horizontally---is produced by the \textbf{equation}
environment. An unnumbered display equation is produced by the
\textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols
and structures available in \LaTeX\@; this section will just
give a couple of examples of display equations in context.
First, consider the equation, shown as an inline equation above:
\begin{equation}
  \lim_{n\rightarrow \infty}x=0
\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}
  \sum_{i=0}^{\infty} x + 1
\end{displaymath}
and follow it with another numbered equation:
\begin{equation}
  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\section{Figures}

The ``\verb|figure|'' environment should be used for figures. One or more images can be placed within a figure. If your figure contains third-party material, you must clearly identify it as such, as shown in the example below.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{sample-franklin}
  \caption{1907 Franklin Model D roadster. Photograph by Harris \& Ewing, Inc. [Public domain], via Wikimedia Commons. (\url{https://goo.gl/VLCRBB}).}
  \Description{The 1907 Franklin Model D roadster.}
\end{figure}

Your figures should contain a caption which describes the figure to the reader. Figure captions go below the figure. Your figures should {\bf also} include a description suitable for screen readers, to assist the visually-challenged to better understand your work.

Figure captions are placed {\it below} the figure.


\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's references is strongly recommended. Authors' names should be complete --- use full first names (``Donald E. Knuth'') not initials (``D. E. Knuth'') --- and the salient identifying features of a reference should be included: title, year, volume, number, pages, article DOI, etc. 

The bibliography is included in your source document with these two commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|'' suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of ACM publications have citations and references formatted in the ``author year'' style; for these exceptions, please include this command in the {\bf preamble} (before ``\verb|\begin{document}|'') of your \LaTeX\ source: 
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}

Some examples.  A paginated journal article \cite{Abril07}, an enumerated journal article \cite{Cohen07}, a reference to an entire issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a monograph/whole book in a series (see 2a in spec. document)
\cite{Harel79}, a divisible-book such as an anthology or compilation \cite{Editor00} followed by the same example, however we only output the series if the volume number is given \cite{Editor00a} (so Editor00a's series should NOT be present since it has no vol. no.),
a chapter in a divisible book \cite{Spector90}, a chapter in a divisible book in a series \cite{Douglass98}, a multi-volume work as book \cite{Knuth97}, an article in a proceedings (of a conference, symposium, workshop for example) (paginated proceedings article) \cite{Andler79}, a proceedings article with all possible elements \cite{Smith10}, an example of an enumerated proceedings article \cite{VanGundy07}, an informally published work \cite{Harel78}, a doctoral dissertation \cite{Clarkson85}, a master's thesis: \cite{anisi03}, an online document / world wide web resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05} and (Case 3) a patent \cite{JoeScientist001}, work accepted for publication \cite{rous08}, 'YYYYb'-test for prolific author \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might contain 'duplicate' DOI and URLs (some SIAM articles) \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton: multi-volume works as books \cite{MR781536} and \cite{MR781537}. A couple of citations with DOIs: \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.



\section{SIGCHI Extended Abstracts}

The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and not in Word) produces a landscape-orientation formatted article, with a wide left margin. Three environments are available for use with the ``\verb|sigchi-a|'' template style, and produce formatted output in the margin:
\begin{itemize}
\item {\verb|sidebar|}:  Place formatted text in the margin.
\item {\verb|marginfigure|}: Place a figure in the margin.
\item {\verb|margintable|}: Place a table in the margin.
\end{itemize}

%
% The acknowledgments section is defined using the "acks" environment (and NOT an unnumbered section). This ensures
% the proper identification of the section in the article metadata, and the consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

% 
% If your work has an appendix, this is the place to put it.
\appendix

\section{Useful web references}


\subsection{Introduction to Deep Learning}

% Sutton and Barto
\noindent \url{http://incompleteideas.net/book/the-book.html}

\noindent \url{https://markus-x-buchholz.medium.com/deep-reinforcement-learning-introduction-deep-q-network-dqn-algorithm-fb74bf4d6862}

\noindent \url{https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html}

\noindent \url{https://www.guru99.com/reinforcement-learning-tutorial.html}

\noindent \url{https://deepmind.com/blog/article/deep-reinforcement-learning}

\noindent \url{https://smartlabai.medium.com/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc}

\noindent \url{https://en.wikipedia.org/wiki/Deep_reinforcement_learning#Off-policy_reinforcement_learning}


\subsection{DQN}

\noindent \url{https://pytorch.org/tutorials/intermediate/reinforcement\_q\_learning.html}

\noindent \url{https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda}

\noindent \url{https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial}

\noindent \url{https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc}

\noindent \url{https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb}

\noindent \url{https://en.wikipedia.org/wiki/Q-learning}

\noindent \url{https://openai.com/blog/openai-baselines-dqn/}


\subsection{General Algorithms and Repositories}

\noindent \url{https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch}

\noindent \url{https://github.com/openai/baselines}


\subsection{Reference Papers and Surveys}

\noindent \url{https://spinningup.openai.com/en/latest/spinningup/keypapers.html}

\noindent \url{https://arxiv.org/abs/1906.10025}



\subsection{Latex}

\noindent \url{https://www.latextemplates.com/template/acm-publications}

\noindent \url{http://math.mit.edu/~dspivak/files/symbols-all.pdf}


\end{document}
